{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/andres/coding/challenge/challenge_code'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''This is a ipython notebook that I used to test the cleaning function'''\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime as dt\n",
    "from utils import get_logger\n",
    "from environment import *\n",
    "from datetime import datetime\n",
    "\n",
    "#initializing the logger\n",
    "logger = get_logger('data_filter')\n",
    "\n",
    "def clean_data() -> pd.DataFrame:\n",
    "    '''\n",
    "    Cleaning each dataframe before merging.\n",
    "    we will keep the attribute 'fuente' to created the computed\n",
    "    tables with it but well make sure to erase it after merging\n",
    "    and computation'''\n",
    "    logger.info('Starting to clean data')\n",
    "    def rename_columns(df):\n",
    "        \"\"\"\n",
    "        renaming the columns of the data frames\n",
    "        \"\"\"\n",
    "        new_columns = ['cod_localidad',\n",
    "                'id_provincia',\n",
    "                'id_departamento',\n",
    "                'categoria',\n",
    "                'provincia',\n",
    "                'localidad',\n",
    "                'nombre',\n",
    "                'domicilio',\n",
    "                'codigo_postal',\n",
    "                'numero_telefono',\n",
    "                'mail',\n",
    "                'web',\n",
    "                'fuente']\n",
    "        df.columns = [new_columns]\n",
    "\n",
    "    ### loading the files using global variables from web_data.p\n",
    "    time_now = datetime.now()       \n",
    "    subfolder = datetime.strftime(time_now, \"%Y-%m\")\n",
    "    file_name_time = datetime.strftime(time_now, \"%d-%m-%Y\")\n",
    "\n",
    "    logger.info('Loading files')\n",
    "    df_museos = pd.read_csv('data_csv/museos/'+ subfolder+'/' + file_name_time + '.csv')\n",
    "    df_cines = pd.read_csv('data_csv/salas de cine/'+ subfolder+'/' + file_name_time + '.csv')\n",
    "    df_bib = pd.read_csv('data_csv/bibliotecas/'+ subfolder+'/' + file_name_time +'.csv')\n",
    "    logger.info('Finished loading files into DataFrames')\n",
    "\n",
    "    ## Deleting the columns we dont need from the dataframes\n",
    "    df_museos.drop(df_museos.columns[\n",
    "        [3,4,10,12,16,17,18,19,21,22,23]\n",
    "        ], axis=1, inplace=True)\n",
    "    df_cines.drop(df_cines.columns[\n",
    "        [3,6,10,12,16,17,18,19,21,22,23,24,25]\n",
    "        ], axis=1, inplace=True)\n",
    "    df_bib.drop(df_bib.columns[\n",
    "        [3,5,7,11,13,17,18,19,20,22,23,24]\n",
    "        ], axis=1, inplace=True)\n",
    "\n",
    "    ## renaming the columns of the dataframse\n",
    "    rename_columns(df_museos)\n",
    "    rename_columns(df_cines)\n",
    "    rename_columns(df_bib)\n",
    "    logger.info('Finished homogenizing the dataframes')\n",
    "    \n",
    "\n",
    "    ###### First type of Table ########\n",
    "    ### Concatenating the cleaned dataframes into an uniform single one\n",
    "    df_combi = pd.concat([df_museos, df_cines, df_bib])\n",
    "\n",
    "\n",
    "    #### Second type of table ########\n",
    "    '''\n",
    "    Creating 3 new tables with computated information by using group by. \n",
    "    The challenge info said it was one table but it makes more \n",
    "    sense like this in my humble opinion :).\n",
    "    '''\n",
    "    print(df_combi.head(10))\n",
    "    df_cat_total = df_combi.groupby('categoria', as_index=False)['cod_localidad'].count()\n",
    "    df_cat_total.columns = ['categoria', 'total']\n",
    "    df_fuente_total = df_combi.groupby('fuente', as_index=False )['cod_localidad'].count()\n",
    "    df_fuente_total.columns = ['fuente', 'total']\n",
    "    df_prov_cat_total = df_combi.groupby(['provincia', 'categoria'], as_index=False)['cod_localidad'].count()\n",
    "    df_prov_cat_total.columns = ['provincia', 'categoria', 'total']\n",
    "\n",
    "\n",
    "    # Erasing the attirubute 'fuente' because it is not needed \n",
    "    # a category that should be in the final merged dataframe\n",
    "    df_combi.drop(['fuente'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    #### Thirth type of table ########\n",
    "    # We use the raw cines dataframe to compute some values by\n",
    "    # using the group by to answear some questions about cinemas \n",
    "    # in Argentina\n",
    "\n",
    "    df_cines2 = pd.read_csv('cines/2022-07/26-07-2022.csv')\n",
    "    df_comp = df_cines2.groupby('Provincia', as_index=False)['Pantallas','Butacas', 'espacio_INCAA'].sum()\n",
    "    df_comp['espacio_INCAA'] = df_cines2.groupby('Provincia', as_index=False)['espacio_INCAA'].count()['espacio_INCAA']\n",
    "    ### rename columns to fit the sql table\n",
    "    df_comp.columns = ['provincia', 'num_pantallas', 'num_butacas', 'num_incaa']\n",
    "\n",
    "    # Creating a dict object to pass the info to the database pusher function\n",
    "    df_pairings = {\n",
    "        #table name: dataframe\n",
    "        'registros_combi': df_combi,\n",
    "        'cines': df_comp,\n",
    "        'categoria_total': df_cat_total,\n",
    "        'fuentes_total': df_fuente_total,\n",
    "        'categoria_provincia_total': df_prov_cat_total\n",
    "    }\n",
    "    logger.info('Finished cleaning data :)')\n",
    "    return df_pairings\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    clean_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Cleaning each data  ame before merging\n",
    "we will keep the attribute 'fuente' to created the computed\n",
    "tables with it but well make sure to erase it after merging and computation'''\n",
    "df_museos = pd.read_csv('../data_csv/museos/2022-07/30-07-2022.csv')\n",
    "## Deleting the columns we dont need\n",
    "df_museos.drop(df_museos.columns[[3,4,10,12,16,17,18,19,21,22,23]], axis=1, inplace=True)\n",
    "## renaming the columns\n",
    "new_name = ['cod_localidad',\n",
    "            'id_provincia',\n",
    "            'id_departamento',\n",
    "            'categoria',\n",
    "            'provincia',\n",
    "            'localidad',\n",
    "            'nombre',\n",
    "            'domicilio',\n",
    "            'codigo_postal',\n",
    "            'numero_telefono',\n",
    "            'mail',\n",
    "            'web',\n",
    "            'fuente']\n",
    "df_museos.columns = new_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## cleaning cines dataset\n",
    "df_cines = pd.read_csv('../data_csv/salas de cine/2022-07/30-07-2022.csv')\n",
    "### Deleting the columns we dont need\n",
    "df_cines.drop(df_cines.columns[[3,6,10,12,16,17,18,19,21,22,23,24,25]], axis=1, inplace=True)\n",
    "### renaming the columns\n",
    "df_cines.columns = new_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Cod_Loc', 'IdProvincia', 'IdDepartamento', 'Categoría', 'Provincia',\n",
      "       'Localidad', 'Nombre', 'Domicilio', 'CP', 'Teléfono', 'Mail', 'Web',\n",
      "       'Fuente'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['cod_localidad', 'id_provincia', 'id_departamento', 'categoria',\n",
       "       'provincia', 'localidad', 'nombre', 'domicilio', 'codigo_postal',\n",
       "       'numero_telefono', 'mail', 'web', 'fuente'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## cleaning bibliotecas dataset \n",
    "df_bib = pd.read_csv('../data_csv/bibliotecas/2022-07/30-07-2022.csv')\n",
    "df_bib.drop(df_bib.columns[[3,5,7,11,13,17,18,19,20,22,23,24]], axis=1, inplace=True)\n",
    "## rename the columns\n",
    "print(df_bib.columns)\n",
    "df_bib.columns = new_name\n",
    "df_bib.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Concadenating the cleaned dataframes\n",
    "df_combi = pd.concat([df_museos, df_cines, df_bib])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating 3 new tables with computated information. The challenge info\n",
    "### said it was one table but it makes more sense like this in my humble opinion\n",
    "df_cat_total = df_combi.groupby('categoria', as_index=False)['cod_localidad'].count()\n",
    "df_cat_total.columns = ['categoria', 'total']\n",
    "df_fuente_total = df_combi.groupby('fuente', as_index=False )['cod_localidad'].count()\n",
    "df_fuente_total.columns = ['fuente', 'total']\n",
    "df_prov_cat_total = df_combi.groupby(['provincia', 'categoria'], as_index=False)['cod_localidad'].count()\n",
    "df_prov_cat_total.columns = ['provincia', 'categoria', 'total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Erasing the attirubute 'fuente' because it is not needed for the tables anymore\n",
    "df_combi.drop(['fuente'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Datos con lo de los cines.\n",
    "df_cines2 = pd.read_csv('cines/2022-07/26-07-2022.csv')\n",
    "df_comp = df_cines2.groupby('Provincia', as_index=False)['Pantallas','Butacas', 'espacio_INCAA'].sum()\n",
    "df_comp['espacio_INCAA'] = df_cines2.groupby('Provincia', as_index=False)['espacio_INCAA'].count()['espacio_INCAA']\n",
    "### rename columns to fit sql table\n",
    "df_comp.columns = ['provincia', 'num_pantallas', 'num_butacas', 'num_incaa']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Now we will export the dataframe info to the sql server in docker.\n",
    "'''\n",
    "## Creating the connection to the server\n",
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('postgresql://docker:docker@localhost:5432/challenge_db')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_prov_cat_total.to_sql('categoria_provincia_total', engine, if_exists='append', index=False)\n",
    "df_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### add a time stamp to all the dataframes\n",
    "### Table - Datafram pairing\n",
    "df_pairings = {\n",
    "    #table name: dataframe\n",
    "    'registros_combi': df_combi,\n",
    "    'cines': df_comp,\n",
    "    'categoria_total': df_cat_total,\n",
    "    'fuentes_total': df_fuente_total,\n",
    "    'categoria_provincia_total': df_prov_cat_total\n",
    "}\n",
    "for value in df_pairings.values():\n",
    "    value['fecha_subida'] = dt.now().strftime('%Y-%m-%d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## populating the tablereplaces\n",
    "for table, dataf in df_pairings.items():\n",
    "    dataf.to_sql(table, engine, if_exists='append', index=False)\n",
    "    print(f'{table} populated')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "95c7506c8f8d91b5a85a354a8d0eccc2efc54cbb7157a8737604e7d7e34f8141"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
